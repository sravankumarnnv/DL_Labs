{"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle competition for Assignment 1","metadata":{"id":"DHNAq9QI7vQn"}},{"cell_type":"markdown","source":"### <span style=\"color:#0b486b\"> II.0 Running on Google Colab</span> <span style=\"color:red\"></span>\nYou will need to download relevant files to run this notebook on Google Colab.","metadata":{"id":"uIg_vVMU_pU4"}},{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install gdown\n!pip install torchsummary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gdown --fuzzy https://drive.google.com/file/d/1aEkxNWaD02Z8ZNvZzeMefUoY97C-3wTG/view?usp=drive_link","metadata":{"id":"LUu6Lil9FUsi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5cd5a3d8-3e55-4d3c-e187-c3dd8fab51c1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -oq Animals_Dataset.zip","metadata":{"id":"_ikNF3Vj_0nT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Commented out IPython magic to ensure Python compatibility.\nimport os\nimport requests\nimport tarfile\nimport time\n\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport PIL.Image\nimport pathlib\nfrom torchsummary import summary\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torchvision.models import resnet50, resnet101, densenet161, mobilenet_v3_large, efficientnet_b3, efficientnet_b4\n\n\n# # Set device to TPU\n# device = xm.xla_device()\n# print(f'Training on device: {device}')\n\n# Check if CUDA is available\ntrain_on_gpu = torch.cuda.is_available()\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iyPfpTo7eJh5","outputId":"6e889a7e-8577-44d4-badb-69c1b3a11cbe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set seeds for reproducibility\ndef seed_all(seed=1029):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nseed_all(1029)\n\n# Initialize TensorBoard for advanced monitoring\nwriter = SummaryWriter('runs/experiment')","metadata":{"id":"m8b7i8gsdZbA","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocess","metadata":{"id":"TJZYOkYKFUsj"}},{"cell_type":"code","source":"data_dir = \"./FIT5215_Dataset\"\n\ntransform = transforms.Compose([\n    transforms.Resize((64, 64)),  # Resize the image\n    transforms.RandomHorizontalFlip(),  # Flip horizontally\n    transforms.RandomRotation(5),  # Reduced rotation\n    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),  # Milder jitter\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize\n    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=0),  # Random Erasing with a 50% chance\n])\n\n\n# Load the dataset using torchvision.datasets.ImageFolder and apply transformations\ndataset = datasets.ImageFolder(data_dir, transform=transform)\n\n# Split the dataset into training and validation sets\ntrain_size = int(0.9 * len(dataset))\nvalid_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, valid_size])\n\n# Create DataLoader for training and validation\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n\nprint(\"Number of instances in train_set:\", len(train_dataset))\nprint(\"Number of instances in val_set:\", len(val_dataset))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"usWhMCu3c1ZB","outputId":"bf44bb10-5e1b-4287-8869-0cc72426e6f8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = ['birds', 'bottles', 'breads', 'butterflies', 'cakes', 'cats', 'chickens', 'cows', 'dogs', 'ducks',\n                  'elephants', 'fishes', 'handguns', 'horses', 'lions', 'lipsticks', 'seals', 'snakes', 'spiders', 'vases']","metadata":{"id":"E-oV3_Aje1aW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# obtain one batch of training images\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\nimages = images.numpy() # convert images to numpy for display","metadata":{"id":"y7ffmpEje3Wy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\ndef imshow(img):\n    img = img / 2 + 0.5  # unnormalize\n    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n\ndef visualize_data(images, categories, images_per_row = 8):\n    class_names = ['birds', 'bottles', 'breads', 'butterflies', 'cakes', 'cats', 'chickens', 'cows', 'dogs', 'ducks',\n                  'elephants', 'fishes', 'handguns', 'horses', 'lions', 'lipsticks', 'seals', 'snakes', 'spiders', 'vases']\n    n_images = len(images)\n    n_rows = math.ceil(float(n_images)/images_per_row)\n    fig = plt.figure(figsize=(1.5*images_per_row, 1.5*n_rows))\n    fig.patch.set_facecolor('white')\n    for i in range(n_images):\n        plt.subplot(n_rows, images_per_row, i+1)\n        plt.xticks([])\n        plt.yticks([])\n        imshow(images[i])\n        class_index = categories[i]\n        plt.xlabel(class_names[class_index])\n    plt.show()","metadata":{"id":"UCDDpWCne-6V","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_data(images, labels)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":871},"id":"b4sw7qzSfDF_","outputId":"09296cb4-c82f-4b93-8d0f-4dbd8f8c2114","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_class = {}\nfor _,outs in dataset:\n    labels = class_names[outs]\n    if labels not in count_class:\n        count_class[labels] = 0\n    count_class[labels] += 1\ncount_class","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rta0go_CgB1q","outputId":"f93224d9-ce56-4b69-9df7-71cadf985a54","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Models","metadata":{"tags":[],"id":"FENpITvPGQea"}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"61q0etHjgafH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define custom Label Smoothing Loss\nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n\n    def forward(self, x, target):\n        with torch.no_grad():\n            true_dist = torch.zeros_like(x)\n            true_dist.fill_(self.smoothing / (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * F.log_softmax(x, dim=-1), dim=-1))\n\n# Initialize Label Smoothing Loss\nloss_fn = LabelSmoothingLoss(classes=20, smoothing=0.05)","metadata":{"id":"9aWroPr6HwxM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_loss(model, loss_fn, loader):\n  loss = 0\n  # Set model to eval mode for inference\n  model.eval()\n  with torch.no_grad():  # No need to track gradients for validation\n    for (batchX, batchY) in loader:\n      # Move data to the same device as the model\n      batchX, batchY = batchX.to(device).type(torch.float32), batchY.to(device).type(torch.long)\n      loss += loss_fn(model(batchX), batchY)\n  # Set model back to train mode\n  model.train()\n  return float(loss)/len(loader)","metadata":{"id":"xH0gwrbvGQea","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_acc(model, loader):\n    correct = 0\n    totals = 0\n    # Set model to eval mode for inference\n    model.eval()\n    for (batchX, batchY) in loader:\n        # Move batchX and batchY to the same device as the model\n        batchX, batchY = batchX.to(device).type(torch.float32), batchY.to(device)\n        outputs = model(batchX)  # feed batch to the model\n        totals += batchY.size(0)  # accumulate totals with the current batch size\n        predicted = torch.argmax(outputs.data, 1)  # get the predicted class\n        # Move batchY to the same device as predicted for comparison\n        correct += (predicted == batchY).sum().item()\n    return correct / totals","metadata":{"id":"TR4P9aAzGQea","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Early Stopping Implementation\nclass EarlyStopping:\n    def __init__(self, patience=5, delta=0.001):\n        self.patience = patience\n        self.delta = delta\n        self.best_loss = float('inf')\n        self.counter = 0\n        self.early_stop = False\n\n    def __call__(self, val_loss):\n        if val_loss < self.best_loss - self.delta:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n                \n# Initialize Early Stopping\nearly_stopping = EarlyStopping(patience=10, delta=0.001)\n\n# Mix-Up Implementation\ndef mixup_data(x, y, alpha=0.2):\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size).to(device)\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\n# Training Loop with the modified implementation\ndef fit(model=None, train_loader=None, valid_loader=None, optimizer=None, num_epochs=50, verbose=True):\n    model.to(device)\n    history = {'val_loss': [], 'val_acc': [], 'train_loss': [], 'train_acc': []}\n\n    for epoch in range(num_epochs):\n        start = time.time()\n        model.train()\n        train_loss, train_correct = 0.0, 0\n          # For GPU and CPU\n        for (X, y) in train_loader:\n            X, y = X.to(device), y.to(device)\n            optimizer.zero_grad()\n            outputs = model(X)\n            loss = loss_fn(outputs, y)\n            loss.backward()\n#         # For TPU     \n#         for data, target in train_loader:\n#             data, target = data.to(device), target.to(device)\n#             output = model(data)\n#             loss = criterion(output, target)\n#             loss.backward()\n#             xm.optimizer_step(optimizer)  # Update parameters and sync with TPU\n\n            # Gradient Clipping (optional)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n            # Update model parameters\n            optimizer.step()\n\n            train_loss += loss.item()\n            train_correct += (outputs.argmax(1) == y).type(torch.float).sum().item()\n\n        # Calculate losses and accuracies for the epoch\n        val_loss = compute_loss(model, loss_fn, valid_loader)\n        val_acc = compute_acc(model, valid_loader)\n        train_loss /= len(train_loader.dataset)\n        train_acc = train_correct / len(train_loader.dataset)\n\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n\n        # Update the scheduler based on validation loss\n        scheduler.step(val_loss)\n\n        # Log data to TensorBoard (optional)\n        writer.add_scalar('Loss/train', train_loss, epoch)\n        writer.add_scalar('Loss/val', val_loss, epoch)\n        writer.add_scalar('Accuracy/train', train_acc, epoch)\n        writer.add_scalar('Accuracy/val', val_acc, epoch)\n        \n        # Check for early stopping (if implemented)\n        early_stopping(val_loss)\n        if early_stopping.early_stop:\n            print(\"Early stopping at epoch:\", epoch + 1)\n            break\n\n        end = time.time()\n        if verbose:\n            print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss:.4f} - Train acc: {train_acc*100:.2f}% - Val loss: {val_loss:.4f} - Val acc: {val_acc*100:.2f}% - Time: {end - start:.2f}s\")\n\n    writer.close()  # Close TensorBoard writer if used\n    return history","metadata":{"id":"zCrHCWoCGQea","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here are some example CNNs for the demonstration purpose. You should use your own developed network for the competition.","metadata":{"id":"Dam5W5St2uid"}},{"cell_type":"code","source":"class CnnModel(nn.Module):\n\tdef __init__(self):\n\t\tsuper().__init__(num_classes=20)\n\t\tself.network = nn.Sequential(\n\t\t\tnn.Conv2d(3, 32, kernel_size=3, padding=1),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n\t\t\tnn.ReLU(),\n\t\t\tnn.MaxPool2d(2, 2), # output: 64 x 48 x 48\n\n\t\t\tnn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n\t\t\tnn.ReLU(),\n\t\t\tnn.MaxPool2d(2, 2), # output: 128 x 24 x 24\n\n\t\t\tnn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n\t\t\tnn.ReLU(),\n\t\t\tnn.MaxPool2d(2, 2), # output: 256 x 12 x 12\n\t\t\tnn.Conv2d(256,512, kernel_size=3, stride=1, padding=1),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n\t\t\tnn.ReLU(),\n\t\t\tnn.MaxPool2d(2, 2), # output: 512 x 6 x 6\n\t\t\tnn.Flatten(),\n\t\t\tnn.Linear(512*6*6, 1024),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Linear(1024, 512),\n\t\t\tnn.ReLU(),\n\t\t\tnn.Linear(512, 20))\n\n\tdef forward(self, xb):\n\t\treturn self.network(xb)\n\n\ndef conv_block(in_channels, out_channels, pool=False):\n\tlayers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n\t\t\t  nn.BatchNorm2d(out_channels),\n\t\t\t  nn.ReLU(inplace=True)]\n\tif pool: layers.append(nn.MaxPool2d(2))\n\treturn nn.Sequential(*layers)\n\nclass ResNet9(nn.Module):\n\tdef __init__(self, in_channels=3, num_classes=10):\n\t\tsuper().__init__()\n\n\t\tself.conv1 = conv_block(in_channels, 64,pool=True)\n\t\tself.conv2 = conv_block(64, 128, pool=True) # output: 128 x 24 x 24\n\t\tself.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n\n\t\tself.conv3 = conv_block(128, 256, pool=True) # output: 256 x 12 x 12\n\t\tself.conv4 = conv_block(256, 512, pool=True) # output: 512 x 6 x 6\n\t\tself.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n\n\t\tself.classifier = nn.Sequential(nn.MaxPool2d(6),\n\t\t\t\t\t\t\t\t\t\tnn.Flatten(),\n\t\t\t\t\t\t\t\t\t\tnn.Dropout(0.2),\n\t\t\t\t\t\t\t\t\t\tnn.Linear(512, num_classes))\n\n\tdef forward(self, xb):\n\t\tout = self.conv1(xb)\n\t\tout = self.conv2(out)\n\t\tout = self.res1(out) + out\n\t\tout = self.conv3(out)\n\t\tout = self.conv4(out)\n\t\tout = self.res2(out) + out\n\t\tout = self.classifier(out)\n\t\treturn out\n\n# Define Models (example CNN for demonstration purposes)\nclass MiniVGG(nn.Module):\n    def __init__(self, num_classes=20):\n        super().__init__()\n        layers = [\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # [32, 32, 32]\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),  # [32, 32, 32]\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2),  # [32, 16, 16]\n            nn.Dropout(p=0.25),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # [64, 16, 16]\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # [64, 16, 16]\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # [64, 16, 16]\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2),  # [64, 8, 8]\n            nn.Dropout(p=0.25),\n            nn.Flatten(1),\n            nn.LazyLinear(512),\n            nn.ReLU(inplace=True),\n            nn.LazyLinear(num_classes),\n        ]\n        self.block = nn.ModuleList(layers)\n\n    def forward(self, x):\n        for layer in self.block:\n            x = layer(x)\n        return x","metadata":{"id":"MmPbreigIJ5t","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the Model","metadata":{"id":"m8-d-P6pGQea"}},{"cell_type":"markdown","source":"### Model Selection","metadata":{"id":"9bzAenofGQea"}},{"cell_type":"code","source":"# Models defined in code\n# model = ResNet9().to(device)\n# model = MiniVGG(20).to(device)\n# model = CnnModel().to(device)\n\n\n\n# Now, initialize the scheduler using the defined optimizer\n\n\n\n# Define the model you want to use by uncommenting one of the following:\n# Model 1: ResNet-50\n# model = resnet50(pretrained=True)\n# model.fc = nn.Linear(model.fc.in_features, 20)  # Adjusting the final layer to match the number of classes (e.g., 20)\n\n# Model 2: ResNet-101\n# model = resnet101(pretrained=True)\n# model.fc = nn.Linear(model.fc.in_features, 20)  # Adjusting the final layer to match the number of classes (e.g., 20)\n\n# Model 3: DenseNet-161\n# model = densenet161(pretrained=True)\n# model.classifier = nn.Linear(model.classifier.in_features, 20)  # Adjust final classifier to match number of classes\n\n# Model 4: MobileNet V3 Large\n# model = mobilenet_v3_large(pretrained=True)\n# model.classifier[3] = nn.Linear(model.classifier[3].in_features, 20)  # Adjusting final layer for number of classes\n\n# Model 5: EfficientNet-B3\n# model = efficientnet_b3(pretrained=True)\n# model.classifier[1] = nn.Linear(model.classifier[1].in_features, 20)  # Adjusting the final layer for number of classes\n\n# Model 6: EfficientNet-B4\nmodel = efficientnet_b4(pretrained=True)\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, 20)  # Adjusting the final layer for number of classes\n\n# Move the model to the device (GPU/CPU)\nmodel = model.to(device)","metadata":{"tags":[],"id":"FRAl8Iy1GQea","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <span style=\"color:#0b486b\">4. Declaring the Loss, Optimizer, learning rate and Training the Model </span>","metadata":{"id":"L23sZjWhGQeb"}},{"cell_type":"code","source":"from torch import optim\noptim_dict = {\"Adam\":optim.Adam, \"Adadelta\":optim.Adadelta, \"Adagrad\":optim.Adagrad,\n              \"Adamax\":optim.Adamax, \"AdamW\": optim.AdamW, \"ASGD\":optim.ASGD,\n              \"NAdam\":optim.NAdam, \"RMSprop\":optim.RMSprop, \"RAdam\":optim.RAdam,\n              \"Rprop\": optim.Rprop, \"SGD\":optim.SGD}\n\n\n# Loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\n\n# Define the optimizer\n\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4) # Directly imported from library\n\n# Initialize the scheduler\n# scheduler = StepLR(optimizer, step_size=15, gamma=0.5) # StepLR\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6) # ReduceLROnPlateau scheduler\n\n#dnn_model = model\n# Training and validation\nhistory = fit(model=model, train_loader=train_loader, valid_loader=val_loader, optimizer=optimizer, num_epochs=50, verbose=True)","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"TwNV7mPdGQeb","outputId":"2f0a8149-3bbe-42d7-f5e7-e602d42375df","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <span style=\"color:#0b486b\">5. Visualizing the Performance and Loss Objective Function </span>\n\nThere are four keys in the history dictionary: `train_loss` and `val_loss` measure the loss on the training set and the validation set, respectively, while `train_acc` and `val_acc` measure the accuracy on the training set and the validation set.  \nThe following figure visualize all four metrics with two y-axes, losses (blue lines, in descending) and accuracies (red lines, in asending)\n","metadata":{"id":"O3-WS91yTvEL"}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\nhis = history\nfig = plt.figure(figsize=(8, 5))\nax = fig.add_subplot(111)\nln1 = ax.plot(his['train_loss'], 'r--',label='train_loss')\nln2 = ax.plot(his['val_loss'], 'b-',label='val_loss')\nax.set_ylabel('loss', color='blue')\nax.tick_params(axis='y', colors=\"blue\")\n\nlns = ln1 + ln2\nlabels = [l.get_label() for l in lns]\nax.legend(lns, labels, loc=7)\nplt.grid(True)\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":465},"id":"6ZVEdlONTuJR","outputId":"ae62c791-e5da-4795-eedf-ca79afdfe135","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate model on the testing set, get the csv file and upload to kaggle website.","metadata":{"id":"qNjeT-EyiRx3"}},{"cell_type":"code","source":"!gdown --fuzzy https://drive.google.com/file/d/1gntVodsAzZntMb1qW2UCns-RdpckfQh0/view?usp=sharing","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yTXWjsYsp857","outputId":"fc27a381-1b2f-4420-e96d-978599c1194d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -q test_set.zip","metadata":{"id":"L8G8nvBKqGP3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_dir = \"./test_set\"\n\n# We resize the images to [3,64,64]\ntransform = transforms.Compose([transforms.Resize((64,64)),  #resises the image so it can be perfect for our model.\n                                      #transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n                                      #transforms.RandomRotation(4),     #Rotates the image to a specified angel\n                                      #transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)), #Performs actions like zooms, change shear angles.\n                                      #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Set the color params\n                                      transforms.ToTensor(), # convert the image to tensor so that it can work with torch\n                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize the images, each R,G,B value is normalized with mean=0.5 and std=0.5\n                                      ])\n\n\n# Load the dataset using torchvision.datasets.ImageFolder and apply transformations\ntestset = datasets.ImageFolder(test_data_dir, transform=transform)\n\nprint(\"Number of instance in test_set: %s\" % len(testset))\n\ntest_loader = DataLoader(testset, batch_size=500, shuffle=False)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z47SOTvls3RM","outputId":"a58b7856-256a-4a38-fac1-c5b62af544e4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport pandas as pd\n\ndef save_prediction_to_csv(model, loader, device, output_file=\"submission.csv\"):\n    model.eval()\n    predictions = []\n    image_ids = []\n    df = {\n    \"ID\": [],\n    \"Label\": []\n    }\n    total = 0\n    with torch.no_grad():\n        for i, (batchX, batchY) in enumerate(loader):\n            batchX, batchY = batchX.to(device), batchY.to(device)\n            outputs = model(batchX.float())  # Convert to float32 and feed batch to the model\n            predicted = torch.argmax(outputs, dim=1)  # Get the predicted class\n            total += predicted.size(0)\n            for ids, pred in enumerate(predicted):\n                label = class_names[pred.to(device).item()]\n                df[\"ID\"].append(i*500+ids)\n                df[\"Label\"].append(label)\n    df[\"ID\"] = [i for i in range(total)]\n    # Create a DataFrame\n    df = pd.DataFrame(df)\n    # Save to CSV\n    df.to_csv(output_file, index=False)","metadata":{"id":"180qpEBIFUsl","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_prediction_to_csv(model, test_loader, device)","metadata":{"id":"KR3gRHtsGTOm","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Upload result to kaggle competition","metadata":{"id":"D3UicCvcFUsl"}},{"cell_type":"markdown","source":"## Regsiter Kaggle account using your private gmail\n## Join the competition\n## Upload the submission.csv file to the kaggle website to get your results","metadata":{"id":"lc9yWR-XFUsl"}},{"cell_type":"code","source":"","metadata":{"id":"f4x1lE5gGQeb"},"execution_count":null,"outputs":[]}]}